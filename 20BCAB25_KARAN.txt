20BACB25
S KARAN KUMAR

#1
import numpy as np
#1
a=np.array([[1,2,3,4,5,6],[2,4,6,8,10,12],[3,6,9,12,15,18],[4,8,12,16,20,24],[5,10,15,20,25,30],[6,12,18,24,30,36]])
print(a)
#2
b=np.array([[1,2,3,4,5,6],[2,4,6,8,10,12],[3,6,9,12,15,18],[4,8,12,16,20,24],[5,10,15,20,25,30],[6,12,18,24,30,36]]).T
print(b)
#3
c=np.array([[1,2,3],[4,5,6]])
d=np.array([[2,4],[3,6],[4,8]])
print(c)
print(d)
#4
print(c.shape)
print(d.shape)
#5
Z=np.array([np.zeros(3)]*3)
for i in range(len(c)):
    for j in range(len(d[1])):
        for k in range(len(d)):
            Z[i][j] += c[i][k] * d[k][j]
print(Z)
#6
p=np.array([[3,6,9],[2,4,6],[4,8,7]])
q=np.array([[9,8,4],[5,2,8],[7,5,1]])
print(p+q)
#7
f=np.array([[1,3],[4,2]])
print(f)
#8
g=np.array([[1,3],[4,2]]).T
print(g)
#9
I=np.array([[1,0],[0,1]])
print(I)
#10
X=f@I
print("A.I = ",X)
Y=I@f
print("I.A = ",Y)
print(" Therefore, A.I = I.A") 
#11
h=np.array([[4,8],[7,5]])
print(h)
#12
print(h*f)
#13
print(np.multiply(f,h)) 
#14
s=np.linalg.det(f)
print("The determinant of A is",s)
if s!=0:
    print("The inverse is ",np.linalg.inv(f))
else:
    print("Inserve does not exist")

#2(plots)
import pandas as pd 
import numpy as np
import os 
import seaborn as sns
import matplotlib.pyplot as plt
os.chdir("C:/Users/Deepak SK/Desktop/3rd sem/datasets")
iris = pd.read_csv('Iris.csv')
iris.head()
print(iris.head())
print(iris.describe())
sns.countplot(x ='Species', data = iris)
plt.show()
sns.scatterplot('SepalLengthCm','SepalWidthCm', hue= 'Species', data = iris)
plt.show()
sns.pairplot(iris.drop(['Id'],axis =1),hue= 'Species', height=2)
plt.show()
sns.boxenplot()
plt.show()
sns.heatmap(iris.corr(), data = iris)
plt.show()
x = iris.corr(method= 'pearson')
print(x)
#3(regression)

import pandas as pd
import numpy as np
import seaborn as sns
from scipy.stats import skew
import os

import matplotlib.pyplot as plt
plt.style.use("ggplot")
plt.rcParams['figure.figsize']=(12,8)
#LOADE THE DATA
os.chdir("F:/3rd sem/datasets")
advert=pd.read_csv('Advertising.csv')
print(advert.head())
advert.info()
#relations berween Features and reaponse
sns.pairplot(advert,x_vars=['TV','radio','newspaper'],y_vars='sales', height=7, aspect=0.7);
#Multiple Linear Regression - Estimating Coefficients
from sklearn.linear_model import LinearRegression

# create X and y
feature_cols = ['TV', 'radio', 'newspaper']
X = advert[feature_cols]
y = advert.sales

# instantiate and fit
lm1 = LinearRegression()
lm1.fit(X, y)

# print the coefficients
print(lm1.intercept_)
print(lm1.coef_)
# pair the feature names with the coefficients
list(zip(feature_cols, lm1.coef_))
sns.heatmap(advert.corr(), annot=True)
#feature seletion
from sklearn.metrics import r2_score

lm2 = LinearRegression().fit(X[['TV', 'radio']], y)
lm2_preds = lm2.predict(X[['TV', 'radio']])

print("R^2: ", r2_score(y, lm2_preds))

lm3 = LinearRegression().fit(X[['TV', 'radio', 'newspaper']], y)
lm3_preds = lm3.predict(X[['TV', 'radio', 'newspaper']])

print("R^2: ", r2_score(y, lm3_preds))


from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = advert[['TV', 'radio', 'newspaper']]
y = advert.sales

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)

lm4 = LinearRegression()
lm4.fit(X_train, y_train)
lm4_preds = lm4.predict(X_test)

print("RMSE :", np.sqrt(mean_squared_error(y_test, lm4_preds)))
print("R^2: ", r2_score(y_test, lm4_preds))

X = advert[['TV', 'radio']]
y = advert.sales

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)

lm5 = LinearRegression()
lm5.fit(X_train, y_train)
lm5_preds = lm5.predict(X_test)

print("RMSE :", np.sqrt(mean_squared_error(y_test, lm5_preds)))
print("R^2: ", r2_score(y_test, lm5_preds))

from yellowbrick.regressor import PredictionError, ResidualsPlot
visualizer = PredictionError(lm5)

visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
visualizer.poof() 

visualizer = ResidualsPlot(lm5)
visualizer.fit(X_train, y_train)  
visualizer.score(X_test, y_test) 
visualizer.poof()
#4
import numpy as np
x= np.array([1,2,3,4,5,6,7])
print(x[1])
print(x[3])
print(x[6])
print(np.shape(x))
print(x)
y=np.array([[1,2,3,4,5,6,7]]).T
print(y)
print(np.shape(y))
a=np.array([[1,2,3,4],[9,8,7,6],[3,4,5,6],[6,2,6,4]])
print(a)
print(a[0,:])
print(a[:,0])
z=np.transpose(a)
print(z)
s=np.zeros((7,7))
print(s)
#3 dimensional array with 1s
h=np.ones((3,3))
print(h)
#5
from sklearn.datasets import load_digits
from sklearn.decomposition import FactorAnalysis
import os
import pandas as pd
import numpy as np

os.chdir("C:/Users/Deepak SK/Desktop/3rd sem/datasets")
d1 = pd.read_csv("covid_19_india.csv")
print(d1.head())
print(d1.dtypes)
print(d1.info())
X, _ = load_digits(return_X_y=True)
fa_Analysis = FactorAnalysis(n_components=8, random_state=123)
X_fa_Analysis  = fa_Analysis.fit_transform(X)
print(X_fa_Analysis.shape)
#6
getwd()
setwd("C:/Users/Deepak SK/Desktop/3rd sem/datasets")
data=read.csv("CarPrice_Assignment (1).csv")
View(data)
head(data)
mean(data$enginesize)
mean(data$horsepower)
t.test(data$enginesize,mu=100)
t.test(data$horsepower,mu=100)
#7
LOGISTIC REGRESSION

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

data=pd.read_csv('Telecom_Data.csv')
data.info()
regressor variables 
x = data.iloc[:, 0:20].values
print(x)
regressed variables
y = data.iloc[:, 20].values
print(y)
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)
classifier = LogisticRegression(random_state = 0)
classifier.fit(xtrain, ytrain)
y_pred = classifier.predict(xtest)
cm = confusion_matrix(ytest, y_pred)
print ("Confusion Matrix : \n", cm)

#8
CLUSTERING

from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage 

dataset = pd.read_csv('CarPrice_Assignment_FA.csv')
dataset.drop(['car_ID','CarName'],axis=1,inplace=True)
dataset.info()
df = dataset.iloc[:, [8,9]].values
Z = linkage(df, method = "ward")
dendro = dendrogram(Z)
plt.title('Dendogram')
plt.ylabel('Euclidean distance')
plt.show()
ac = AgglomerativeClustering(n_clusters=4, affinity="euclidean", linkage="ward")
labels = ac.fit_predict(df)
plt.figure(figsize = (8,5))
plt.scatter(df[labels == 0,0] , df[labels == 0,1], c= 'red')
plt.scatter(df[labels == 1,0] , df[labels == 1,1], c= 'blue')
plt.scatter(df[labels == 2,0] , df[labels == 2,1], c= 'green')
plt.scatter(df[labels == 3,0] , df[labels == 3,1], c= 'black')
plt.scatter(df[labels == 4,0] , df[labels == 4,1], c= 'orange')
plt.show()
