import numpy as np

x=np.array([1,2,3,4,5,6,7])

print(x)

print(x[1],x[3³],x[6])

print(np.shape(x))

y=x.T

print(y)

a=np.array([[1,2,3,4,5,],[5,6,7,8,9,],[9,8,7,6,5],[1,3,5,7,9]]) print(a[0,0], a[0,1],a[0,2],a[0,3], a[0,4])

print(a)

print(np.shape(a))





b=a. T

print (b)

print(np.shape(b))

print(a[0,0], a[1,0],a[2,0],a[3,0])

k=np.zeros((7))

print (k)

s=np.ones ((3))

print(s)


# -*- coding: utf-8 -*-
"""
Created on Fri Oct  1 11:14:49 2021

@author: prem
"""

import numpy as np
X=np.array([[1,2,3],[3,2,1],[9,10,7]])
print(X)
Y=np.array([[1,2,3],[3,2,1],[9,10,7]]).T
print(X)
A=np.array([[0]*3])
print(A)
print(np.zeros((4,8)))
print(np.ones(4))
print(X)
print(Y)
print(X+Y)
print(np.shape(X))
I=np.array([[10,20,30,40,50,60,70]])
print(I)
print(I[0,1],I[0,3],I[0,6])
print(np.shape(I))
print(I.T)
V=np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]])
print(V)
print(np.shape(V))
print(V.T)
print(V[:,0])
print(V[0,:])
print(np.zeros(7))
print(np.ones(3))

# -*- coding: utf-8 -*-
"""
Created on Fri Jan 14 07:15:19 2022

@author: prem
"""


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix


os.chdir("C:/Users/prem
/Documents")
data=pd.read_csv('Telecom_Data.csv')

       
data.info()


# regressor variables 
x = data.iloc[:, 0:20].values
#print(x)
  
# regressed variables
y = data.iloc[:, 20].values
#print(y)


xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)




classifier = LogisticRegression(random_state = 0)
classifier.fit(xtrain, ytrain)

#y_pred = classifier.predict(xtest)

#cm = confusion_matrix(ytest, y_pred)
  
#print ("Confusion Matrix : \n", cm)



# -*- coding: utf-8 -*-
"""
Created on Tue Jan  4 17:17:43 2022

@author: prem
"""

import pandas as pd
from sklearn.datasets import load_iris
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
from factor_analyzer.factor_analyzer import calculate_kmo
import matplotlib.pyplot as plt
import os
import seaborn as sns
import numpy as np

os.chdir("C:/Users/prem/Documents")
df=pd.read_csv('CarPrice_Assignment_FA.csv')
df.info()
df.drop(['car_ID','CarName'],axis=1,inplace=True)
df.info()
# Converting the categorical data into continous was done manually using FIND AND REPLACE in MS Excel.

# Checking the correlation
#x= df.corr(method= 'pearson')
#print(x)
#sns.heatmap(df.corr(method='pearson'),data=df)
#plt.show()
# Bartlett’s test
#chi_square_value,p_value=calculate_bartlett_sphericity(df)
#print(chi_square_value, p_value)
# Not sure how to interpret this. 


# Kaiser-Meyer-Olkin (KMO) Test
kmo_all,kmo_model=calculate_kmo(df)
print(kmo_model)
# KMO values range between 0 and 1. Value of KMO less than 0.5 is considered inadequate.
# The overall KMO for our data is 0.78, which is pretty good. 
# This value indicates that we can proceed with our planned factor analysis.


#Choosing the number of factors
# Create factor analysis object and perform factor analysis
#fa = FactorAnalyzer()
#fa.analyze(df, 25, rotation=None)
#Check Eigenvalues
#ev, v = fa.get_eigenvalues()
#print(ev)

fa = FactorAnalyzer()
fa.fit(df)
eigen_values, vectors = fa.get_eigenvalues()
print(vectors)
# 3 eigen values are greater than 1 therefore,
# NUMBER OF FACTORS = 3


# Create scree plot using matplotlib
plt.scatter(range(1,df.shape[1]+1),vectors)
plt.plot(range(1,df.shape[1]+1),vectors)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
# It is understandable from the scree plot that the number of factors 3 or 4.

# Create factor analysis object and perform factor analysis
fa = FactorAnalyzer()
fa.set_params(n_factors=6, rotation='varimax')
fa.fit(df)
loadings = fa.loadings_
print(loadings)


# Get variance of each factors
print(fa.get_factor_variance())
# It is in the below format 
#                      Factor 1       Factor2       Factor3 
# SS Loadings
# Proportion Var
# Cummulative Var

# Total 58% cumulative Variance is explained by the 3 factors.
# -*- coding: utf-8 -*-
"""
Created on Fri Nov 12 07:23:56 2021

@author: prem
"""
# EDA and linear regression for two pair of variables
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import datasets
import sklearn



os.chdir("C:/Users/prem/Documents")
mtcars=pd.read_csv('CarPrice_Assignment.csv')
#print(mtcars.describe())
mtcars.info()

# 1. EDA and visualisation 

#print(mtcars.describe())

#sns.countplot('doornumber',data=mtcars)
#plt.show()

#plt.hist('cylindernumber',data = mtcars)
#plt.show()

#x= mtcars.corr(method= 'pearson')
#print(x)

#sns.heatmap(mtcars.corr(method='pearson').drop(['car_ID','symboling'],axis=1).drop(['car_ID','symboling'],axis=0),data=mtcars)
#sns.show()

#df = pd.DataFrame(mtcars,columns=['cylindernumber','horsepower'])
#plt.bar(df['cylindernumber'], df['horsepower'])
#plt.title('Cylinder number vs Horsepower', fontsize=14)
#plt.xlabel('CYlinder Number', fontsize=14)
#plt.ylabel('Horse Power', fontsize=14)
#plt.show()


#sns.pairplot(mtcars)
#plt.show()

#sns.boxplot(y='compressionratio',x='fueltype',data=mtcars)
#plt.show()


#2. Regression on one variable 
#(a) Regression on one variable for negative correlation
#X=mtcars[['highwaympg']]
#Y=mtcars[['horsepower']]
#reg=linear_model.LinearRegression()
#reg.fit(X,Y)
#print(reg.coef_)
#sns.regplot(X,Y)
#plt.show()

#(b) Regression on one variable for positive correlation
X=mtcars[['wheelbase']]
Y=mtcars[['carlength']]
reg=linear_model.LinearRegression()
reg.fit(X,Y)
print(reg.coef_)
print(reg.intercept_) #IMP
sns.regplot(X,Y)
plt.show()

#(c) Regression on one variable with no correlation
#X=mtcars[['stroke']]
#Y=mtcars[['price']]
#reg=linear_model.LinearRegression()
#reg.fit(X,Y)
#print(reg.coef_)
#sns.regplot(X,Y)
#plt.show()



#3. Regression on multiple variables
X=mtcars[['horsepower','curbweight']]
Y=mtcars[['price']]
reg=linear_model.LinearRegression()
reg.fit(X,Y)
print(reg.coef_)

# complete credit to the internet for the below code
df2 = pd.DataFrame(mtcars,columns=['horsepower','curbweight','price'])
import statsmodels.formula.api as smf
model = smf.ols(formula='price ~ horsepower + curbweight', data=df2)
results_formula = model.fit()
results_formula.params


## Prepare the data for Visualization

x_surf, y_surf = np.meshgrid(np.linspace(df2.horsepower.min(), df2.horsepower.max(), 100),np.linspace(df2.curbweight.min(), df2.curbweight.max(), 100))
onlyX = pd.DataFrame({'horsepower': x_surf.ravel(), 'curbweight': y_surf.ravel()})
fittedY=results_formula.predict(exog=onlyX)



## convert the predicted result in an array
fittedY=np.array(fittedY)




# Visualize the Data for Multiple Linear Regression

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df2['horsepower'],df2['curbweight'],df2['price'],c='red', marker='o', alpha=0.5)
ax.plot_surface(x_surf,y_surf,fittedY.reshape(x_surf.shape), color='b', alpha=0.3)
ax.set_xlabel('Horsepower')
ax.set_ylabel('Curbweight')
ax.set_zlabel('Price')
plt.show()
# -*- coding: utf-8 -*-
"""
Created on Thu Oct 28 08:47:41 2021

@author: prem
"""

import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt

os.chdir("C:/Users/prem/Documents")
iris = pd.read_csv('Iris.csv')
print(iris.head())
print(iris.describe())
sns.countplot(x='Species', data = iris)
plt.show()
#sns.scatterplot('SepalLengthCm','SepalWidthCm', hue='Species',data = iris)
#plt.show()
#sns.pairplot(iris.drop(['Id'],axis =1), hue= 'Species', height= 2)
#plt.show()
#sns.heatmap(iris.corr(), data = iris)
#plt.show()
#x= iris.corr(method= 'pearson')
#print(x)
#sns.heatmap(iris.corr(method='pearson').drop(['Id'],axis=1).drop(['Id'],axis=0))

# -*- coding: utf-8 -*-
"""
Created on Fri Oct  1 11:14:49 2021

@author: prem
"""

import numpy as np
X=np.array([[1,2,3],[3,2,1],[9,10,7]])
print(X)
Y=np.array([[1,2,3],[3,2,1],[9,10,7]]).T
print(X)
A=np.array([[0]*3])
print(A)
print(np.zeros((4,8)))
print(np.ones(4))
print(X)
print(Y)
print(X+Y)
print(np.shape(X))
I=np.array([[10,20,30,40,50,60,70]])
print(I)
print(I[0,1],I[0,3],I[0,6])
print(np.shape(I))
print(I.T)
V=np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]])
print(V)
print(np.shape(V))
print(V.T)
print(V[:,0])
print(V[0,:])
print(np.zeros(7))
print(np.ones(3))
